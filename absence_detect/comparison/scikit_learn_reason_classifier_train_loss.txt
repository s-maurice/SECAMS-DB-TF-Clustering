Iteration 1, loss = 3.52228401
Iteration 2, loss = 1.49607825
Iteration 3, loss = 1.03037484
Iteration 4, loss = 0.82610464
Iteration 5, loss = 0.74298814
Iteration 6, loss = 0.70755854
Iteration 7, loss = 0.69131523
Iteration 8, loss = 0.68313049
Iteration 9, loss = 0.67869867
Iteration 10, loss = 0.67938198
Iteration 11, loss = 0.67394668
Iteration 12, loss = 0.67257910
Iteration 13, loss = 0.67158377
Iteration 14, loss = 0.67088491
Iteration 15, loss = 0.67045983
Iteration 16, loss = 0.66902281
Iteration 17, loss = 0.61172931
Iteration 18, loss = 0.57304545
Iteration 19, loss = 0.52606966
Iteration 20, loss = 0.47055571
Iteration 21, loss = 0.40319561
Iteration 22, loss = 0.51898065
Iteration 23, loss = 0.63536815
Iteration 24, loss = 0.66557182
Iteration 25, loss = 0.65385688
Iteration 26, loss = 0.66418604
Iteration 27, loss = 0.65999835
Iteration 28, loss = 0.65745518
Iteration 29, loss = 0.65237978
Iteration 30, loss = 0.64862984
Iteration 31, loss = 0.64116182
Iteration 32, loss = 0.63333738
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 2.98369968
Iteration 2, loss = 0.63544196
Iteration 3, loss = 0.59898586
Iteration 4, loss = 0.55799338
Iteration 5, loss = 0.51499855
Iteration 6, loss = 0.45258508
Iteration 7, loss = 0.40086943
Iteration 8, loss = 0.35598608
Iteration 9, loss = 0.32767453
Iteration 10, loss = 0.31534906
Iteration 11, loss = 0.29913140
Iteration 12, loss = 0.29958543
Iteration 13, loss = 0.28593156
Iteration 14, loss = 0.28706336
Iteration 15, loss = 0.28418607
Iteration 16, loss = 0.26439867
Iteration 17, loss = 0.26810928
Iteration 18, loss = 0.25969210
Iteration 19, loss = 0.29990147
Iteration 20, loss = 0.24765320
Iteration 21, loss = 0.24446500
Iteration 22, loss = 0.27443242
Iteration 23, loss = 0.23938157
Iteration 24, loss = 0.27298553
Iteration 25, loss = 0.22172631
Iteration 26, loss = 0.21796300
Iteration 27, loss = 0.24974848
Iteration 28, loss = 0.21009835
Iteration 29, loss = 0.21758566
Iteration 30, loss = 0.24796034
Iteration 31, loss = 0.20577388
Iteration 32, loss = 0.23827867
Iteration 33, loss = 0.20008186
Iteration 34, loss = 0.20835262
Iteration 35, loss = 0.21346577
Iteration 36, loss = 0.21109278
Iteration 37, loss = 0.22431743
Iteration 38, loss = 0.19763084
Iteration 39, loss = 0.21285779
Iteration 40, loss = 0.19741613
Iteration 41, loss = 0.19933308
Iteration 42, loss = 0.22557664
Iteration 43, loss = 0.19390930
Iteration 44, loss = 0.20944704
Iteration 45, loss = 0.18515057
Iteration 46, loss = 0.19147322
Iteration 47, loss = 0.22472600
Iteration 48, loss = 0.18667543
Iteration 49, loss = 0.23446966
Iteration 50, loss = 0.18276913
Iteration 51, loss = 0.18607519
Iteration 52, loss = 0.20827255
Iteration 53, loss = 0.18550696
Iteration 54, loss = 0.18889038
Iteration 55, loss = 0.19079048
Iteration 56, loss = 0.20131135
Iteration 57, loss = 0.18576214
Iteration 58, loss = 0.19582806
Iteration 59, loss = 0.19729811
Iteration 60, loss = 0.20309908
Iteration 61, loss = 0.17820632
Iteration 62, loss = 0.21382732
Iteration 63, loss = 0.18250482
Iteration 64, loss = 0.18763042
Iteration 65, loss = 0.19275274
Iteration 66, loss = 0.17972221
Iteration 67, loss = 0.19267981
Iteration 68, loss = 0.18824370
Iteration 69, loss = 0.18020528
Iteration 70, loss = 0.17704527
Iteration 71, loss = 0.17851892
Iteration 72, loss = 0.22850140
Iteration 73, loss = 0.17098889
Iteration 74, loss = 0.19790600
Iteration 75, loss = 0.17565842
Iteration 76, loss = 0.18426031
Iteration 77, loss = 0.17121534
Iteration 78, loss = 0.20648629
Iteration 79, loss = 0.17164439
Iteration 80, loss = 0.16913565
Iteration 81, loss = 0.17314338
Iteration 82, loss = 0.18234999
Iteration 83, loss = 0.24761805
Iteration 84, loss = 0.16662500
Iteration 85, loss = 0.16670684
Iteration 86, loss = 0.17654039
Iteration 87, loss = 0.20661615
Iteration 88, loss = 0.17486652
Iteration 89, loss = 0.18249746
Iteration 90, loss = 0.17244925
Iteration 91, loss = 0.17834962
Iteration 92, loss = 0.16645734
Iteration 93, loss = 0.20331522
Iteration 94, loss = 0.16570102
Iteration 95, loss = 0.18412015
Iteration 96, loss = 0.17105244
Iteration 97, loss = 0.17260028
Iteration 98, loss = 0.19732801
Iteration 99, loss = 0.16198326
Iteration 100, loss = 0.17216830
Iteration 101, loss = 0.16848973
Iteration 102, loss = 0.17087913
Iteration 103, loss = 0.18103471
Iteration 104, loss = 0.16387159
Iteration 105, loss = 0.18664486
Iteration 106, loss = 0.16288822
Iteration 107, loss = 0.18843483
Iteration 108, loss = 0.16483917
Iteration 109, loss = 0.18316988
Iteration 110, loss = 0.15584960
Iteration 111, loss = 0.15819376
Iteration 112, loss = 0.18323490
Iteration 113, loss = 0.15326938
Iteration 114, loss = 0.14931699
Iteration 115, loss = 0.18478156
Iteration 116, loss = 0.14373641
Iteration 117, loss = 0.15787992
Iteration 118, loss = 0.16736237
Iteration 119, loss = 0.13409008
Iteration 120, loss = 0.13248022
Iteration 121, loss = 0.17918696
Iteration 122, loss = 0.12874548
Iteration 123, loss = 0.12927940
Iteration 124, loss = 0.15547561
Iteration 125, loss = 0.12551811
Iteration 126, loss = 0.13135156
Iteration 127, loss = 0.12860670
Iteration 128, loss = 0.23018432
Iteration 129, loss = 0.12506312
Iteration 130, loss = 0.13459278
Iteration 131, loss = 0.12872288
Iteration 132, loss = 0.20282720
Iteration 133, loss = 0.12503490
Iteration 134, loss = 0.12969941
Iteration 135, loss = 0.19911592
Iteration 136, loss = 0.12636954
Iteration 137, loss = 0.12346039
Iteration 138, loss = 0.15766619
Iteration 139, loss = 0.12392148
Iteration 140, loss = 0.12520318
Iteration 141, loss = 0.16392964
Iteration 142, loss = 0.12529061
Iteration 143, loss = 0.16358829
Iteration 144, loss = 0.12655091
Iteration 145, loss = 0.12435909
Iteration 146, loss = 0.16931491
Iteration 147, loss = 0.12368054
Iteration 148, loss = 0.12713621
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = inf
Iteration 2, loss = 1.22773719
Iteration 3, loss = 0.67068310
Iteration 4, loss = 0.61681528
Iteration 5, loss = 0.58669716
Iteration 6, loss = 0.56713329
Iteration 7, loss = 0.53920260
Iteration 8, loss = 0.51170147
Iteration 9, loss = 0.48705086
Iteration 10, loss = 0.46497002
Iteration 11, loss = 0.43001378
Iteration 12, loss = 0.39911964
Iteration 13, loss = 0.36558080
Iteration 14, loss = 0.33786610
Iteration 15, loss = 0.32040722
Iteration 16, loss = 0.29045835
Iteration 17, loss = 0.28934457
Iteration 18, loss = 0.26752387
Iteration 19, loss = 0.28712506
Iteration 20, loss = 0.24358504
Iteration 21, loss = 0.24692458
Iteration 22, loss = 0.23391891
Iteration 23, loss = 0.29225545
Iteration 24, loss = 0.22039265
Iteration 25, loss = 0.22106777
Iteration 26, loss = 0.29564454
Iteration 27, loss = 0.20726458
Iteration 28, loss = 0.21112685
Iteration 29, loss = 0.21295553
Iteration 30, loss = 0.24320553
Iteration 31, loss = 0.26176209
Iteration 32, loss = 0.19793708
Iteration 33, loss = 0.22264787
Iteration 34, loss = 0.24773190
Iteration 35, loss = 0.19837664
Iteration 36, loss = 0.20463265
Iteration 37, loss = 0.20407018
Iteration 38, loss = 0.19819622
Iteration 39, loss = 0.20304278
Iteration 40, loss = 0.20752182
Iteration 41, loss = 0.23931298
Iteration 42, loss = 0.20177467
Iteration 43, loss = 0.18657169
Iteration 44, loss = 0.23595690
Iteration 45, loss = 0.18291761
Iteration 46, loss = 0.20740926
Iteration 47, loss = 0.25232032
Iteration 48, loss = 0.18043621
Iteration 49, loss = 0.18247410
Iteration 50, loss = 0.19867537
Iteration 51, loss = 0.18665923
Iteration 52, loss = 0.25974095
Iteration 53, loss = 0.17985892
Iteration 54, loss = 0.18211813
Iteration 55, loss = 0.24280210
Iteration 56, loss = 0.18322935
Iteration 57, loss = 0.17945304
Iteration 58, loss = 0.58864910
Iteration 59, loss = 0.18473204
Iteration 60, loss = 0.22835445
Iteration 61, loss = 0.17706921
Iteration 62, loss = 0.18046206
Iteration 63, loss = 0.19932320
Iteration 64, loss = 0.18041566
Iteration 65, loss = 0.18323212
Iteration 66, loss = 0.17721865
Iteration 67, loss = 0.23122187
Iteration 68, loss = 0.20707268
Iteration 69, loss = 0.17629194
Iteration 70, loss = 0.22551403
Iteration 71, loss = 0.17349646
Iteration 72, loss = 0.17392732
Iteration 73, loss = 0.21297110
Iteration 74, loss = 0.17206440
Iteration 75, loss = 0.17550789
Iteration 76, loss = 0.20178885
Iteration 77, loss = 0.16691600
Iteration 78, loss = 0.18135004
Iteration 79, loss = 0.16145024
Iteration 80, loss = 0.18561647
Iteration 81, loss = 0.17729689
Iteration 82, loss = 0.16539241
Iteration 83, loss = 0.14789797
Iteration 84, loss = 0.23261209
Iteration 85, loss = 0.13403467
Iteration 86, loss = 0.14980100
Iteration 87, loss = 0.17508123
Iteration 88, loss = 0.20070118
Iteration 89, loss = 0.15553071
Iteration 90, loss = 0.20147702
Iteration 91, loss = 0.14784254
Iteration 92, loss = 0.14584658
Iteration 93, loss = 0.17318684
Iteration 94, loss = 0.17149890
Iteration 95, loss = 0.21604369
Iteration 96, loss = 0.17033227